import streamlit as st
import pandas as pd
import json, io, zipfile, tempfile, os, re, time, hashlib
from datetime import datetime
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from cozepy import Coze as coze, TokenAuth, WorkflowEventType, COZE_CN_BASE_URL

def tab8_content():
    """
    è‡ªåŠ¨åŒ–ç¿»è¯‘è¿­ä»£å·¥ä½œå°ï¼š
    1. ä¸Šä¼ åŸæ–‡ã€è¯‘æ–‡
    2. é…ç½®æ ‡ç­¾ï¼ˆè‡ªå®šä¹‰æ ‡ç­¾ã€å¯è¿­ä»£æ ‡ç­¾ï¼‰
    3. é€‰æ‹©å¯¼å‡ºæ¡ä»¶
    4. ç‚¹å‡»"å¼€å§‹è‡ªåŠ¨åŒ–"åæ¯ 5 ç§’è‡ªåŠ¨å¾ªç¯ï¼šç­›é€‰ â†’ Workflow â†’ è¿­ä»£ â†’ æ›´æ–°è¯‘æ–‡
    5. åœæ­¢æ¡ä»¶ï¼šç”¨æˆ·ç‚¹åœæ­¢/å¯¼å‡ºã€æˆ–å¾…ç¿»è¯‘å­—æ®µ â‰¤ 50
    """
    
    # åˆå§‹åŒ–ä¼šè¯å˜é‡
    if "auto_running" not in st.session_state:
        st.session_state.auto_running = False
    if "auto_logs" not in st.session_state:
        st.session_state.auto_logs = []
    if "auto_translation_dict" not in st.session_state:
        st.session_state.auto_translation_dict = {}
    if "auto_pending_keys" not in st.session_state:
        st.session_state.auto_pending_keys = []
    if "translation_file_hash" not in st.session_state:
        st.session_state.translation_file_hash = None
    if "auto_loop_count" not in st.session_state:
        st.session_state.auto_loop_count = 0

    st.header("è‡ªåŠ¨åŒ–ç¿»è¯‘è¿­ä»£")
    st.info("ä¸Šä¼ åŸæ–‡å’Œç¿»è¯‘æ–‡ä»¶åï¼Œè‡ªåŠ¨å¾ªç¯æ‰§è¡Œ Workflow è°ƒç”¨å’Œè¿­ä»£ï¼Œç›´åˆ°è¾¾æˆåœæ­¢æ¡ä»¶ã€‚")

    # ä¸Šä¼ æ–‡ä»¶
    original_file = st.file_uploader("ä¸Šä¼ åŸæ–‡æ–‡ä»¶ (.txt)", type="txt", key="tab8_original")
    translation_file = st.file_uploader("ä¸Šä¼ ç¿»è¯‘æ–‡ä»¶ (.txt)", type="txt", key="tab8_translation")

    # å…¬ç”¨è§£æå‡½æ•°ï¼ˆå¤ç”¨ tab7 é€»è¾‘ï¼‰
    def parse_txt(file):
        result = {}
        text = file.getvalue().decode("utf-8-sig", errors="replace")
        for line in text.splitlines():
            if "=" in line:
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip()
                if key:
                    result[key] = value
        return result

    def parse_workflow_results(workflow_results):
        parsed = {}
        if not workflow_results:
            return parsed
        for batch in workflow_results:
            if not isinstance(batch, dict):
                continue
            items = batch.get("download_url", [])
            if not isinstance(items, list):
                continue
            for raw_item in items:
                if not raw_item or not isinstance(raw_item, str):
                    continue
                text = raw_item.strip()
                if text.startswith("[") and text.endswith("]"):
                    try:
                        decoded = json.loads(text)
                        if isinstance(decoded, list):
                            for sub in decoded:
                                _extract_kv_relaxed(sub, parsed)
                            continue
                    except Exception:
                        pass
                _extract_kv_relaxed(text, parsed)
        return parsed

    def _extract_kv_relaxed(text, parsed_dict):
        if not text or "=" not in text:
            return
        key, value = text.split("=", 1)
        key = key.strip()
        value = value.strip()
        if not key or not value:
            return
        parsed_dict[key] = value

    def calculate_single_status(orig_text, trans_text, statuses):
        orig_len = len(orig_text)
        trans_len = len(trans_text)
        if orig_len == 0:
            return "åŸæ–‡ä¸ºç©º"
        ratio = (trans_len - orig_len) / orig_len
        ratio = round(ratio, 4)
        for s in statuses:
            s_min = float("-inf") if s.get("min") is None else s["min"]
            s_max = float("inf") if s.get("max") is None else s["max"]
            if s_min <= ratio <= s_max:
                return s["name"]
        return "æœªåˆ†ç±»"

    def calculate_length_status(original_dict, translation_dict, statuses):
        data = []
        for key, orig_text in original_dict.items():
            trans_text = translation_dict.get(key, "")
            orig_len = len(orig_text)
            trans_len = len(trans_text)
            if orig_len == 0:
                ratio = None
                ratio_percent = ""
            else:
                ratio = (trans_len - orig_len) / orig_len
                ratio = round(ratio, 4)
                ratio_percent = f"{ratio*100:.2f}%"
            status = "åŸæ–‡ä¸ºç©º" if orig_len == 0 else None
            if ratio is not None:
                for s in statuses:
                    s_min = float("-inf") if s.get("min") is None else s["min"]
                    s_max = float("inf") if s.get("max") is None else s["max"]
                    if s_min <= ratio <= s_max:
                        status = s["name"]
                        break
                if status is None:
                    status = "æœªåˆ†ç±»"
            data.append({
                "ç¼–å·": key,
                "åŸæ–‡": orig_text,
                "è¯‘æ–‡": trans_text,
                "åŸæ–‡é•¿åº¦": orig_len,
                "è¯‘æ–‡é•¿åº¦": trans_len,
                "æ¯”å€¼": ratio,
                "æ¯”å€¼(%)": ratio_percent,
                "æ ‡ç­¾": status
            })
        return pd.DataFrame(data)

    def process_iteration(original_dict, translation_dict, iteration_dict, iterable_labels, custom_statuses):
        iteration_stats = {
            "total_in_iteration": 0,
            "matched_in_original": 0,
            "updated_translations": 0,
            "skipped_not_iterable": 0,
            "iteration_labels_distribution": {}
        }
        updated_records = []
        iteration_labels_count = {}
        if not iteration_dict:
            return translation_dict, updated_records, iteration_stats, iteration_labels_count
        
        iteration_stats["total_in_iteration"] = len(iteration_dict)
        iterable_labels_norm = [label.strip() for label in iterable_labels]
        before_update_translation = translation_dict.copy()

        for key, iteration_value in iteration_dict.items():
            if key in original_dict:
                iteration_stats["matched_in_original"] += 1
                original_text = original_dict[key]
                iteration_label = calculate_single_status(original_text, iteration_value, custom_statuses)
                iteration_labels_count[iteration_label] = iteration_labels_count.get(iteration_label, 0) + 1
                if iteration_label in iterable_labels_norm:
                    translation_dict[key] = iteration_value
                    iteration_stats["updated_translations"] += 1
                else:
                    iteration_stats["skipped_not_iterable"] += 1

        iteration_stats["iteration_labels_distribution"] = iteration_labels_count

        for key, iteration_value in iteration_dict.items():
            if key in original_dict:
                original_text = original_dict.get(key, "")
                old_translation = before_update_translation.get(key, "")
                iteration_label = calculate_single_status(original_text, iteration_value, custom_statuses)
                if iteration_label in iterable_labels_norm:
                    orig_len = len(original_text)
                    old_trans_len = len(old_translation)
                    new_trans_len = len(iteration_value)
                    if orig_len == 0:
                        original_ratio = None
                        original_ratio_pct = ""
                        new_ratio = None
                        new_ratio_pct = ""
                    else:
                        original_ratio = round((old_trans_len - orig_len) / orig_len, 4)
                        original_ratio_pct = f"{original_ratio*100:.2f}%"
                        new_ratio = round((new_trans_len - orig_len) / orig_len, 4)
                        new_ratio_pct = f"{new_ratio*100:.2f}%"
                    new_label = calculate_single_status(original_text, iteration_value, custom_statuses)
                    updated_records.append({
                        "ç¼–å·": key,
                        "åŸæ–‡": original_text,
                        "åŸè¯‘æ–‡": old_translation,
                        "æ–°è¯‘æ–‡": iteration_value,
                        "åŸæ¯”å€¼": original_ratio,
                        "åŸæ¯”å€¼(%)": original_ratio_pct,
                        "æ–°æ¯”å€¼": new_ratio,
                        "æ–°æ¯”å€¼(%)": new_ratio_pct,
                        "æ–°æ ‡ç­¾": new_label
                    })

        return translation_dict, updated_records, iteration_stats, iteration_labels_count

    if original_file and translation_file:
        # è§£æåŸæ–‡
        original_dict = parse_txt(original_file)

        # è§£æè¯‘æ–‡ï¼Œä½¿ç”¨å“ˆå¸Œåˆ¤æ–­æ˜¯å¦ä¸ºæ–°ä¸Šä¼ 
        raw_bytes = translation_file.getvalue()
        file_hash = hashlib.md5(raw_bytes).hexdigest() if raw_bytes is not None else None
        parsed_translation = parse_txt(translation_file)

        # åªæœ‰å½“ session ä¸­æ²¡æœ‰è¯‘æ–‡ï¼Œæˆ–ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ä¸ session ä¸­ä¿å­˜çš„ä¸åŒï¼Œæ‰è¦†ç›– session ä¸­çš„è¯‘æ–‡å­—å…¸
        prev_hash = st.session_state.get("translation_file_hash")
        if prev_hash != file_hash or not st.session_state.get("auto_translation_dict"):
            st.session_state.auto_translation_dict = parsed_translation
            st.session_state.translation_file_hash = file_hash
            # æ–°ä¸Šä¼ æ—¶é‡ç½®è‡ªåŠ¨åŒ–çŠ¶æ€
            st.session_state.auto_running = False
            st.session_state.auto_logs = []
            st.session_state.auto_loop_count = 0

        translation_dict = st.session_state.get("auto_translation_dict", parsed_translation)

        # ---- æ ‡ç­¾é…ç½® ----
        st.subheader("é…ç½®æ ‡ç­¾")
        col1, col2 = st.columns(2)

        with col1:
            st.write("**è‡ªå®šä¹‰æ ‡ç­¾è®¾ç½®**ï¼ˆç”¨äºåˆå§‹ç»Ÿè®¡å’Œå¯¼å‡ºç­›é€‰ï¼‰")
            default_statuses = [
                {"name": "åˆæ ¼", "min": -0.4, "max": 2, "color": "#00A000"},
                {"name": "è¿‡çŸ­", "min": -99999, "max": -0.4, "color": "#0071A6"},
                {"name": "è¿‡é•¿", "min": 2, "max": 99999, "color": "#A60000"}
            ]
            status_count = st.number_input("æ ‡ç­¾æ•°é‡", min_value=1, max_value=10, value=len(default_statuses), step=1, key="tab8_status_count")
            custom_statuses = []
            for i in range(status_count):
                default = default_statuses[i] if i < len(default_statuses) else {"name": f"æ ‡ç­¾{i+1}", "min": -99999, "max": 99999, "color": "#FFFFFF"}
                col_name, col_min, col_max = st.columns([2, 1, 1])
                with col_name:
                    name = st.text_input(f"æ ‡ç­¾{i+1} åç§°", value=default["name"], key=f"tab8_status_name_{i}")
                with col_min:
                    min_val = st.number_input(f"æœ€å°å€¼", value=float(default["min"]), key=f"tab8_status_min_{i}")
                with col_max:
                    max_val = st.number_input(f"æœ€å¤§å€¼", value=float(default["max"]), key=f"tab8_status_max_{i}")
                custom_statuses.append({"name": name.strip(), "min": min_val, "max": max_val, "color": default["color"]})

        with col2:
            st.write("**å¯è¿­ä»£æ ‡ç­¾é€‰æ‹©**ï¼ˆç”¨äºè¿‡æ»¤è¿­ä»£ç»“æœï¼‰")
            iterable_labels = st.multiselect(
                "é€‰æ‹©å“ªäº›æ ‡ç­¾çš„ç»“æœå¯ä»¥è¢«è¿­ä»£",
                options=[s["name"] for s in custom_statuses],
                default=[custom_statuses[0]["name"]],
                key="tab8_iterable_labels"
            )

        # ---- å¯¼å‡ºæ¡ä»¶ ----
        st.subheader("é€‰æ‹©å¯¼å‡ºæ¡ä»¶ï¼ˆç­›é€‰å¾…ç¿»è¯‘å­—æ®µï¼‰")
        export_checks = {}
        for s in custom_statuses:
            export_checks[s["name"]] = st.checkbox(f"{s['name']}å­—æ®µ", value=(s["name"] in ["è¿‡çŸ­","è¿‡é•¿"]), key=f"tab8_export_{s['name']}")

        # ---- è‡ªåŠ¨åŒ–å‚æ•° ----
        st.subheader("è‡ªåŠ¨åŒ–å‚æ•°")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            loop_interval = st.number_input("å¾ªç¯é—´éš”ï¼ˆç§’ï¼‰", min_value=1, max_value=60, value=5, step=1, key="tab8_loop_interval")
        with col2:
            threshold = st.number_input("åœæ­¢é˜ˆå€¼ï¼ˆå¾…ç¿»è¯‘å­—æ®µæ•°â‰¤æ­¤å€¼æ—¶åœæ­¢ï¼‰", min_value=1, max_value=500, value=50, step=10, key="tab8_threshold")
        with col3:
            token = st.text_input("Token", value="1234567890", key="tab8_token")
        with col4:
            coze_api = st.text_input("å·¥ä½œæµ API", value="7582900707377446975", key="tab8_coze_api")

        st.subheader("ç¿»è¯‘å‚æ•°")
        col1, col2 = st.columns(2)
        with col1:
            target_language = st.text_input("ç›®æ ‡è¯­è¨€", value="è¾“å…¥è¯­è¨€", key="tab8_target_language")
        with col2:
            terminology = st.text_input("æœ¯è¯­åº“é“¾æ¥ï¼ˆå¯é€‰ï¼‰", value="", key="tab8_terminology")

        # ---- è‡ªåŠ¨åŒ–æ—¥å¿—å®¹å™¨ ----
        log_container = st.container()
        with log_container:
            st.subheader("è‡ªåŠ¨åŒ–æ‰§è¡Œæ—¥å¿—")
            log_area = st.empty()

        # ---- åˆå§‹åŒ– Workflow å®¢æˆ·ç«¯ ----
        COZE_TOKEN = token
        WORKFLOW_ID = coze_api
        coze_client = coze(auth=TokenAuth(token=COZE_TOKEN), base_url=COZE_CN_BASE_URL)

        # ---- è‡ªåŠ¨åŒ–æ ¸å¿ƒé€»è¾‘ ----
        def auto_iterate_loop(original_dict, translation_dict, custom_statuses, iterable_labels, export_checks, loop_interval, threshold, language="es", terminology=""):
            """
            è‡ªåŠ¨åŒ–å¾ªç¯ï¼šç­›é€‰ â†’ Workflow â†’ è¿­ä»£
            è¿”å›æœ€ç»ˆæ›´æ–°çš„ translation_dict å’Œæ—¥å¿—åˆ—è¡¨
            """
            logs = []
            loop_count = 0

            while st.session_state.auto_running:
                loop_count += 1
                logs.append(f"\n{'='*60}")
                logs.append(f"ç¬¬ {loop_count} è½®è¿­ä»£å¼€å§‹ (æ—¶é—´: {datetime.now().strftime('%H:%M:%S')})")
                logs.append(f"{'='*60}")

                # è®¡ç®—å¾…ç¿»è¯‘å­—æ®µ
                df_result_runtime = calculate_length_status(original_dict, translation_dict, custom_statuses)
                export_df_runtime = df_result_runtime[df_result_runtime["æ ‡ç­¾"].isin([name for name, checked in export_checks.items() if checked])]
                
                pending_count = len(export_df_runtime)
                logs.append(f"å½“å‰å¾…ç¿»è¯‘å­—æ®µæ•°: {pending_count}")

                # æ£€æŸ¥åœæ­¢æ¡ä»¶ï¼šå¾…ç¿»è¯‘å­—æ®µ â‰¤ é˜ˆå€¼
                if pending_count <= threshold:
                    logs.append(f"âœ… å¾…ç¿»è¯‘å­—æ®µæ•° ({pending_count}) â‰¤ é˜ˆå€¼ ({threshold})ï¼Œè‡ªåŠ¨åœæ­¢")
                    st.session_state.auto_running = False
                    break

                # æ„å»ºæ‰¹æ¬¡
                export_keys = list(export_df_runtime["ç¼–å·"])
                field_objects = [f"{k}={original_dict[k]}" for k in export_keys]
                batch_size = 10
                batches = [field_objects[i:i+batch_size] for i in range(0, len(field_objects), batch_size)]
                total_batches = len(batches)
                logs.append(f"æ„å»º {total_batches} æ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š 10 æ¡")

                # å¹¶è¡Œè°ƒç”¨ Workflow
                def run_batch(batch, batch_index):
                    results = []
                    try:
                        stream = coze_client.workflows.runs.stream(
                            workflow_id=WORKFLOW_ID,
                            parameters={
                                "url": batch,
                                "language": language,
                                "terminology": terminology
                            }
                        )
                        for event in stream:
                            if event.event == WorkflowEventType.MESSAGE:
                                content = getattr(event.message, "content", None)
                                if content:
                                    try:
                                        results.append(json.loads(content))
                                    except Exception:
                                        results.append(content)
                    except Exception as e:
                        logs.append(f"âŒ æ‰¹æ¬¡ {batch_index+1} è°ƒç”¨å¤±è´¥: {e}")
                    return batch_index, results

                # åˆ›å»ºå®æ—¶è¿›åº¦å®¹å™¨
                progress_container = st.container()
                progress_bar = progress_container.progress(0)
                status_text = progress_container.empty()
                batch_results_text = progress_container.empty()
                
                all_results = [None] * total_batches
                batch_summaries = []
                
                with ThreadPoolExecutor(max_workers=min(total_batches, 5)) as executor:
                    futures = {executor.submit(run_batch, batch, idx): idx for idx, batch in enumerate(batches)}
                    completed = 0
                    
                    for future in futures:
                        idx, results = future.result()
                        all_results[idx] = results
                        completed += 1
                        
                        # æ›´æ–°è¿›åº¦
                        progress_percent = int((completed / total_batches) * 100)
                        progress_bar.progress(progress_percent)
                        
                        # ç»Ÿè®¡ç»“æœ
                        result_count = len(results)
                        batch_summaries.append(f"æ‰¹æ¬¡ {idx+1}: {result_count} æ¡")
                        
                        # æ˜¾ç¤ºå®æ—¶çŠ¶æ€
                        active_tasks = total_batches - completed
                        status_text.text(f"â³ å·²å®Œæˆ: {completed}/{total_batches} | æ´»è·ƒä»»åŠ¡: {active_tasks}")
                        batch_results_text.markdown(
                            f"**æ‰¹æ¬¡è¿›åº¦è¯¦æƒ…**\n\n" + 
                            "\n".join([f"âœ… {s}" for s in batch_summaries]) +
                            f"\n\n**æ€»è®¡è·å¾—: {sum(len(all_results[i]) if all_results[i] else 0 for i in range(len(all_results)))} æ¡**"
                        )

                workflow_results = [item for batch in all_results if batch for item in batch]
                logs.append(f"âœ… Workflow è°ƒç”¨å®Œæˆï¼Œè·å¾— {len(workflow_results)} æ¡ç»“æœ")
                logs.append(f"ğŸ“‹ æ‰¹æ¬¡æ±‡æ€»: {' | '.join(batch_summaries)}")

                # è§£æ Workflow ç»“æœ
                parsed_results = parse_workflow_results(workflow_results)
                logs.append(f"âœ… è§£æç»“æœ: {len(parsed_results)} æ¡æœ‰æ•ˆå†…å®¹")

                # æ‰§è¡Œè¿­ä»£
                if parsed_results:
                    translation_dict, updated_records, iteration_stats, iteration_labels_count = process_iteration(
                        original_dict, translation_dict, parsed_results, iterable_labels, custom_statuses
                    )
                    logs.append(f"ğŸ“Š è¿­ä»£ç»Ÿè®¡:")
                    logs.append(f"  - æ€»æ¡ç›®: {iteration_stats['total_in_iteration']}")
                    logs.append(f"  - åŒ¹é…åŸæ–‡: {iteration_stats['matched_in_original']}")
                    logs.append(f"  - å·²æ›´æ–°: {iteration_stats['updated_translations']}")
                    logs.append(f"  - è¢«è·³è¿‡: {iteration_stats['skipped_not_iterable']}")
                    logs.append(f"  - æ ‡ç­¾åˆ†å¸ƒ: {iteration_stats['iteration_labels_distribution']}")
                    
                    # æ·»åŠ å¯è§†åŒ–ç»§å›½æ¬¡è®¡æ•°å™¨
                    st.session_state.auto_loop_count = loop_count
                    
                    # æ·»åŠ æµæ›™è®¡æ•°
                    col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)
                    with col_stats1:
                        st.metric("æ€»æ¡ç›®", iteration_stats['total_in_iteration'])
                    with col_stats2:
                        st.metric("åŒ¹é…åŸæ–‡", iteration_stats['matched_in_original'])
                    with col_stats3:
                        st.metric("å·²æ›´æ–°", iteration_stats['updated_translations'], delta=f"+{iteration_stats['updated_translations']}")
                    with col_stats4:
                        st.metric("è¢«è·³è¿‡", iteration_stats['skipped_not_iterable'])
                else:
                    logs.append(f"âš ï¸ æœªè·å¾—æœ‰æ•ˆè¿­ä»£å†…å®¹")

                # æ›´æ–°ä¼šè¯
                st.session_state.auto_translation_dict = translation_dict
                st.session_state.auto_loop_count = loop_count

                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦ç‚¹å‡»åœæ­¢ï¼ˆé€šè¿‡è¶…æ—¶æœºåˆ¶ï¼‰
                logs.append(f"ç­‰å¾… {loop_interval} ç§’åè¿›è¡Œä¸‹ä¸€è½®...")
                for i in range(loop_interval):
                    if not st.session_state.auto_running:
                        logs.append("âœ‹ ç”¨æˆ·å·²åœæ­¢è‡ªåŠ¨åŒ–")
                        break
                    time.sleep(1)

            logs.append(f"\n{'='*60}")
            logs.append(f"è‡ªåŠ¨åŒ–å®Œæˆ (æ€»è½®æ•°: {loop_count})")
            logs.append(f"{'='*60}")
            return translation_dict, logs

        # ---- UI æ§åˆ¶ ----
        col1, col2, col3 = st.columns([2, 2, 2])

        with col1:
            if st.button("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–", key="tab8_start"):
                st.session_state.auto_running = True
                st.session_state.auto_logs = []
                st.session_state.auto_loop_count = 0

        with col2:
            if st.button("â¹ï¸ åœæ­¢è‡ªåŠ¨åŒ–", key="tab8_stop"):
                st.session_state.auto_running = False

        with col3:
            if st.button("ğŸ“¥ å¯¼å‡ºæœ€æ–°è¯‘æ–‡å¹¶åœæ­¢", key="tab8_export_stop"):
                st.session_state.auto_running = False

        # ---- æ‰§è¡Œè‡ªåŠ¨åŒ–å¾ªç¯ ----
        if st.session_state.auto_running:
            translation_dict, logs = auto_iterate_loop(
                original_dict,
                st.session_state.auto_translation_dict,
                custom_statuses,
                iterable_labels,
                export_checks,
                loop_interval,
                threshold,
                language=target_language,
                terminology=terminology
            )
            st.session_state.auto_translation_dict = translation_dict
            st.session_state.auto_logs.extend(logs)

        # æ˜¾ç¤ºæ—¥å¿—
        if st.session_state.auto_logs:
            log_text = "\n".join(st.session_state.auto_logs)
            log_area.text_area("æ‰§è¡Œæ—¥å¿—", value=log_text, height=400, disabled=True, key="tab8_log_display")

        # ---- å¯¼å‡ºå½“å‰æœ€æ–°è¯‘æ–‡ ----
        st.subheader("å¯¼å‡ºç»“æœ")
        final_translation_dict = st.session_state.get("auto_translation_dict", translation_dict)
        final_translation_txt = "\n".join([f"{key}={value}" for key, value in final_translation_dict.items()])
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æœ€æ–°ç‰ˆç¿»è¯‘æ–‡ä»¶ (.txt)",
            data=final_translation_txt,
            file_name=f"è‡ªåŠ¨åŒ–ç¿»è¯‘_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            key="tab8_download"
        )

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        st.subheader("å½“å‰ç¿»è¯‘ç»Ÿè®¡")
        df_final = calculate_length_status(original_dict, final_translation_dict, custom_statuses)
        stats_data = []
        for s in custom_statuses:
            count = (df_final["æ ‡ç­¾"] == s["name"]).sum()
            ratio = (count / len(df_final) * 100) if len(df_final) > 0 else 0
            stats_data.append({"æ ‡ç­¾": s["name"], "æ•°é‡": count, "å æ¯”": f"{ratio:.2f}%"})
        stats_df = pd.DataFrame(stats_data)
        st.dataframe(stats_df, use_container_width=True)

        # æ˜¾ç¤ºè‡ªåŠ¨åŒ–å¾ªç¯æ¬¡æ•°
        st.info(f"å·²æ‰§è¡Œå¾ªç¯æ¬¡æ•°: {st.session_state.auto_loop_count}")


