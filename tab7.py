import streamlit as st
import pandas as pd
import json, io, zipfile, subprocess, tempfile, os, re
from datetime import datetime
from collections import OrderedDict
from st_aggrid import AgGrid, GridOptionsBuilder, JsCode, GridUpdateMode, DataReturnMode
from cozepy import Coze as coze, TokenAuth, WorkflowEventType, COZE_CN_BASE_URL
import time as t
import hashlib
from concurrent.futures import ThreadPoolExecutor

def tab7_content():
    if "workflow_results" not in st.session_state:
        st.session_state.workflow_results = []

    if "workflow_raw_events" not in st.session_state:
        st.session_state.workflow_raw_events = []

    if "has_workflow_result" not in st.session_state:
        st.session_state.has_workflow_result = False

    if "iteration_dict" not in st.session_state:
        st.session_state.iteration_dict = {}
    # ä¼šè¯ä¸­æŒä¹…åŒ–å¾…ç¿»è¯‘é˜Ÿåˆ—å’Œæœ€æ–°è¯‘æ–‡ï¼Œé¿å…é‡å¤ä½¿ç”¨æ—§è¯‘æ–‡è¿›è¡Œè¿­ä»£
    if "pending_keys" not in st.session_state:
        st.session_state.pending_keys = []
    if "translation_dict" not in st.session_state:
        st.session_state.translation_dict = {}

    st.header("å·¥ä½œæµæµ‹è¯•")
    st.info("ä¸Šä¼ åŸæ–‡æ–‡ä»¶å’Œç¿»è¯‘æ–‡ä»¶åï¼Œå·¥å…·ä¼šè®¡ç®—æ¯ä¸ªå­—æ®µçš„é•¿åº¦æ¯”å€¼ï¼Œå¹¶æ ‡è®°ä¸ºæ ‡ç­¾ã€‚å¯è‡ªå®šä¹‰æ ‡ç­¾ï¼Œä¹Ÿå¯ä½¿ç”¨é»˜è®¤è¿‡çŸ­/åˆæ ¼/è¿‡é•¿æ ‡ç­¾ã€‚")

    # ä¸Šä¼ åŸæ–‡æ–‡ä»¶å’Œç¿»è¯‘æ–‡ä»¶
    original_file = st.file_uploader("ä¸Šä¼ åŸæ–‡æ–‡ä»¶ (.txt)", type="txt")
    translation_file = st.file_uploader("ä¸Šä¼ ç¿»è¯‘æ–‡ä»¶ (.txt)", type="txt")
    iteration_file = st.file_uploader("ä¸Šä¼ è¿­ä»£æ–‡ä»¶ (.txt, å¯é€‰)", type="txt")

    def parse_txt(file):
        result = {}
        # ä½¿ç”¨ utf-8-sig è‡ªåŠ¨ç§»é™¤ BOMï¼Œå¹¶ç”¨ errors='replace' é˜²æ­¢å¥‡æ€ªç¼–ç æŠ¥é”™
        text = file.getvalue().decode("utf-8-sig", errors="replace")
        for line in text.splitlines():
            if "=" in line:
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.strip()
                if key:  # å¿½ç•¥ç©º key
                    result[key] = value
        return result
    
    def parse_workflow_results(workflow_results):
        """
        å°† workflow è¿”å›çš„ç»“æœè§£ææˆ {ç¼–å·: å†…å®¹} çš„ dict
        è§„åˆ™ï¼š
        - ç­‰å·å·¦è¾¹æ˜¯ç¼–å·ï¼ˆä»»ä½•å­—ç¬¦ï¼‰
        - ç­‰å·å³è¾¹æ˜¯å†…å®¹
        """
        parsed = {}

        if not workflow_results:
            return parsed

        for batch in workflow_results:
            if not isinstance(batch, dict):
                continue

            items = batch.get("download_url", [])
            if not isinstance(items, list):
                continue

            for raw_item in items:
                if not raw_item or not isinstance(raw_item, str):
                    continue

                text = raw_item.strip()

                # â‘  å°è¯•è§£ä¸€å±‚ JSONï¼ˆå¤„ç† ["xxx=yyy"]ï¼‰
                if text.startswith("[") and text.endswith("]"):
                    try:
                        decoded = json.loads(text)
                        if isinstance(decoded, list):
                            for sub in decoded:
                                _extract_kv_relaxed(sub, parsed)
                            continue
                    except Exception:
                        pass  # è§£ä¸å¼€å°±å½“æ™®é€šå­—ç¬¦ä¸²ç»§ç»­

                # â‘¡ æ™®é€šå­—ç¬¦ä¸²
                _extract_kv_relaxed(text, parsed)

        return parsed

    def _extract_kv_relaxed(text, parsed_dict):
        if not text or "=" not in text:
            return

        key, value = text.split("=", 1)

        key = key.strip()
        value = value.strip()

        # å·¦å³éƒ½å¿…é¡»éç©º
        if not key or not value:
            return

        parsed_dict[key] = value

    def process_iteration(original_dict, translation_dict, iteration_dict, iterable_labels, custom_statuses):
        """
        Apply iteration dict to translation_dict according to iterable_labels and custom_statuses.
        Returns: (new_translation_dict, updated_records, iteration_stats, iteration_labels_count)
        """
        iteration_stats = {
            "total_in_iteration": 0,
            "matched_in_original": 0,
            "updated_translations": 0,
            "skipped_not_iterable": 0,
            "iteration_labels_distribution": {}
        }

        updated_records = []
        iteration_labels_count = {}

        if not iteration_dict:
            return translation_dict, updated_records, iteration_stats, iteration_labels_count

        iteration_stats["total_in_iteration"] = len(iteration_dict)
        iterable_labels_norm = [label.strip() for label in iterable_labels]

        before_update_translation = translation_dict.copy()

        for key, iteration_value in iteration_dict.items():
            if key in original_dict:
                iteration_stats["matched_in_original"] += 1
                original_text = original_dict[key]
                iteration_label = calculate_single_status(original_text, iteration_value, custom_statuses)
                iteration_labels_count[iteration_label] = iteration_labels_count.get(iteration_label, 0) + 1

                if iteration_label in iterable_labels_norm:
                    translation_dict[key] = iteration_value
                    iteration_stats["updated_translations"] += 1
                else:
                    iteration_stats["skipped_not_iterable"] += 1
            else:
                # key not in original, skip
                pass

        iteration_stats["iteration_labels_distribution"] = iteration_labels_count

        # build updated_records for display
        for key, iteration_value in iteration_dict.items():
            if key in original_dict:
                original_text = original_dict.get(key, "")
                old_translation = before_update_translation.get(key, "")
                iteration_label = calculate_single_status(original_text, iteration_value, custom_statuses)
                if iteration_label in iterable_labels_norm:
                    orig_len = len(original_text)
                    old_trans_len = len(old_translation)
                    new_trans_len = len(iteration_value)

                    if orig_len == 0:
                        original_ratio = None
                        original_ratio_pct = ""
                        new_ratio = None
                        new_ratio_pct = ""
                    else:
                        original_ratio = round((old_trans_len - orig_len) / orig_len, 4)
                        original_ratio_pct = f"{original_ratio*100:.2f}%"
                        new_ratio = round((new_trans_len - orig_len) / orig_len, 4)
                        new_ratio_pct = f"{new_ratio*100:.2f}%"

                    new_label = calculate_single_status(original_text, iteration_value, custom_statuses)

                    updated_records.append({
                        "ç¼–å·": key,
                        "åŸæ–‡": original_text,
                        "åŸè¯‘æ–‡": old_translation,
                        "æ–°è¯‘æ–‡": iteration_value,
                        "åŸæ¯”å€¼": original_ratio,
                        "åŸæ¯”å€¼(%)": original_ratio_pct,
                        "æ–°æ¯”å€¼": new_ratio,
                        "æ–°æ¯”å€¼(%)": new_ratio_pct,
                        "æ–°æ ‡ç­¾": new_label
                    })

        return translation_dict, updated_records, iteration_stats, iteration_labels_count

    # ---- æ ‡ç­¾è‡ªå®šä¹‰é…ç½® ----
    st.subheader("è‡ªå®šä¹‰æ ‡ç­¾è®¾ç½®ï¼ˆå¯é€‰ï¼‰")
    st.info("å¦‚æœä¸ä¿®æ”¹ï¼Œé»˜è®¤ä½¿ç”¨ï¼šåˆæ ¼ / è¿‡çŸ­ / è¿‡é•¿ æ ‡ç­¾ã€‚")

    default_statuses = [
        {"name": "åˆæ ¼", "min": -0.4, "max": 2, "color": "#00A000"},
        {"name": "è¿‡çŸ­", "min": -99999, "max": -0.4, "color": "#0071A6"},
        {"name": "è¿‡é•¿", "min": 2, "max": 99999, "color": "#A60000"}
    ]

    status_count = st.number_input("æ ‡ç­¾æ•°é‡", min_value=1, max_value=10, value=len(default_statuses), step=1, key="tab1_status_count")
    custom_statuses = []

    st.write("æ ‡ç­¾é…ç½®")
    for i in range(status_count):
        default = default_statuses[i] if i < len(default_statuses) else {"name": f"æ ‡ç­¾{i+1}", "min": -99999, "max": 99999, "color": "#FFFFFF"}
        col1, col2, col3, col4 = st.columns([3,2,2,2])
        with col1:
            name = st.text_input(f"æ ‡ç­¾{i+1} åç§°", value=default["name"])
        with col2:
            min_val = st.number_input(f"æ ‡ç­¾{i+1} æœ€å°å€¼", value=float(default["min"]))
        with col3:
            max_val = st.number_input(f"æ ‡ç­¾{i+1} æœ€å¤§å€¼", value=float(default["max"]))
        with col4:
            color = st.color_picker(f"æ ‡ç­¾{i+1} é¢œè‰²", value=default["color"])
        custom_statuses.append({"name": name.strip(), "min": min_val, "max": max_val, "color": color})

    # ---- å¯è¿­ä»£æ ‡ç­¾é€‰æ‹© ----
    st.subheader("é€‰æ‹©å¯è¿­ä»£æ ‡ç­¾(é»˜è®¤åˆæ ¼)")
    st.info("æ³¨æ„ï¼šç³»ç»Ÿä¼šè®¡ç®—è¿­ä»£æ–‡ä»¶ä¸­æ¯ä¸ªæ¡ç›®çš„æ ‡ç­¾ï¼Œåªæœ‰æ ‡ç­¾åœ¨å¯è¿­ä»£åˆ—è¡¨ä¸­çš„æ¡ç›®æ‰ä¼šè¢«æ›´æ–°åˆ°ç¿»è¯‘æ–‡ä»¶ä¸­ã€‚")
    
    iterable_labels = st.multiselect(
        "é€‰æ‹©å“ªäº›æ ‡ç­¾å¯ä»¥è¢«è¿­ä»£",
        options=[s["name"] for s in custom_statuses],
        default=[custom_statuses[0]["name"]]  # é»˜è®¤ç¬¬ä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯"åˆæ ¼"ï¼‰
    )

    # ----- è®¡ç®—å•ä¸ªæ¡ç›®çš„æ ‡ç­¾å‡½æ•° -----
    def calculate_single_status(orig_text, trans_text, statuses):
        orig_len = len(orig_text)
        trans_len = len(trans_text)

        if orig_len == 0:
            return "åŸæ–‡ä¸ºç©º"

        ratio = (trans_len - orig_len) / orig_len
        ratio = round(ratio, 4)

        for s in statuses:
            s_min = float("-inf") if s.get("min") is None else s["min"]
            s_max = float("inf") if s.get("max") is None else s["max"]
            if s_min <= ratio <= s_max:
                return s["name"]
        
        return "æœªåˆ†ç±»"

    # ----- è®¡ç®—æ¯”å€¼å’Œæ ‡ç­¾çš„å‡½æ•° -----
    def calculate_length_status(original_dict, translation_dict, statuses):
        data = []
        for key, orig_text in original_dict.items():
            trans_text = translation_dict.get(key, "")
            orig_len = len(orig_text)
            trans_len = len(trans_text)

            if orig_len == 0:
                ratio = None
                ratio_percent = ""
            else:
                ratio = (trans_len - orig_len) / orig_len
                ratio = round(ratio, 4)
                ratio_percent = f"{ratio*100:.2f}%"

            status = "åŸæ–‡ä¸ºç©º" if orig_len == 0 else None

            if ratio is not None:
                for s in statuses:
                    s_min = float("-inf") if s.get("min") is None else s["min"]
                    s_max = float("inf") if s.get("max") is None else s["max"]
                    if s_min <= ratio <= s_max:
                        status = s["name"]
                        break
                if status is None:
                    status = "æœªåˆ†ç±»"

            data.append({
                "ç¼–å·": key,
                "åŸæ–‡": orig_text,
                "è¯‘æ–‡": trans_text,
                "åŸæ–‡é•¿åº¦": orig_len,
                "è¯‘æ–‡é•¿åº¦": trans_len,
                "æ¯”å€¼": ratio,
                "æ¯”å€¼(%)": ratio_percent,
                "æ ‡ç­¾": status
            })
        return pd.DataFrame(data)

    # ---- ç»Ÿè®¡ä¿¡æ¯å‡½æ•° ----
    def compute_statistics(df, statuses, total_field="åŸæ–‡"):
        records = []
        total_valid = df[total_field].apply(lambda x: bool(x.strip())).sum()
        records.append({"ç±»å‹": f"{total_field}æœ‰æ•ˆå­—æ®µæ•°é‡", "æ•°é‡": total_valid, "å æ¯”": ""})
        total_trans = df["è¯‘æ–‡"].apply(lambda x: bool(x.strip())).sum()
        records.append({"ç±»å‹": "è¯‘æ–‡æœ‰æ•ˆå­—æ®µæ•°é‡", "æ•°é‡": total_trans, "å æ¯”": ""})
        for s in statuses:
            count = df[df["æ ‡ç­¾"] == s["name"]].shape[0]
            ratio = (count / total_valid * 100) if total_valid else 0
            records.append({"ç±»å‹": s["name"], "æ•°é‡": count, "å æ¯”": f"{ratio:.2f}%"})
        return pd.DataFrame(records)

    if original_file and translation_file:
        original_dict = parse_txt(original_file)

        # è¯»å–ä¸Šä¼ è¯‘æ–‡çš„ raw bytes è®¡ç®—å“ˆå¸Œï¼Œä»¥åŒºåˆ†æ˜¯å¦ä¸ºæ–°ä¸Šä¼ ï¼ˆStreamlit ä¼šåœ¨æ¯æ¬¡ rerun ä¸­é‡æ–°ä¼ å…¥ file uploaderï¼‰
        raw_bytes = translation_file.getvalue()
        file_hash = hashlib.md5(raw_bytes).hexdigest() if raw_bytes is not None else None

        parsed_translation = parse_txt(translation_file)

        # åªæœ‰å½“ session ä¸­æ²¡æœ‰è¯‘æ–‡ï¼Œæˆ–ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ä¸ session ä¸­ä¿å­˜çš„ä¸åŒï¼Œæ‰è¦†ç›– session ä¸­çš„è¯‘æ–‡å­—å…¸
        prev_hash = st.session_state.get("translation_file_hash")
        if prev_hash != file_hash or not st.session_state.get("translation_dict"):
            st.session_state.translation_dict = parsed_translation
            st.session_state.translation_file_hash = file_hash

        # ä½¿ç”¨ä¼šè¯ä¸­çš„è¯‘æ–‡ï¼ˆå¯èƒ½æ˜¯åˆšåˆšè§£æçš„ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¹‹å‰è¿­ä»£åçš„è¯‘æ–‡ï¼‰
        translation_dict = st.session_state.get("translation_dict", parsed_translation)

        # åˆå§‹åŒ–è¿­ä»£ç»Ÿè®¡
        iteration_stats = {
            "total_in_iteration": 0,
            "matched_in_original": 0,
            "updated_translations": 0,
            "skipped_not_iterable": 0,
            "iteration_labels_distribution": {}
        }

        st.write(
            "DEBUG:",
            "iteration_file =", bool(iteration_file),
            "iteration_dict size =", len(st.session_state.iteration_dict),
            "has_workflow_result =", st.session_state.has_workflow_result
        )
        st.subheader("è¿­ä»£æ›´æ–°")
        st.info("éœ€è¦å…ˆä¸Šä¼ è¿­ä»£æ–‡ä»¶ï¼Œæˆ–æˆåŠŸæ‰§è¡Œä¸€æ¬¡ Workflow åæ‰èƒ½ä½¿ç”¨")

        iteration_dict = {}

        if iteration_file:
            iteration_dict = parse_txt(iteration_file)
        else:
            iteration_dict = st.session_state.iteration_dict

        st.write("DEBUG iteration_dict size:", len(iteration_dict))

        if not iteration_dict:
            st.warning("æ²¡æœ‰å¯ç”¨çš„è¿­ä»£å†…å®¹")
        else:
            # ä½¿ç”¨å°è£…å‡½æ•°å¤„ç†è¿­ä»£ï¼Œè¿™æ ·å¯ä»¥è¢« workflow åçš„æŒ‰é’®å¤ç”¨
            translation_dict, updated_records, iteration_stats, iteration_labels_count = process_iteration(
                original_dict, translation_dict, iteration_dict, iterable_labels, custom_statuses
            )

            if iteration_stats["total_in_iteration"] > 0:
                st.success("è¿­ä»£æ–‡ä»¶å¤„ç†å®Œæˆ:")

                # åˆ›å»ºè¿­ä»£åˆ†æè¡¨æ ¼
                iteration_analysis = []
                for label, count in iteration_labels_count.items():
                    percentage = (count / iteration_stats["matched_in_original"] * 100) if iteration_stats["matched_in_original"] > 0 else 0
                    iteration_analysis.append({
                        "æ ‡ç­¾": label,
                        "æ•°é‡": count,
                        "å æ¯”": f"{percentage:.1f}%",
                        "æ˜¯å¦å¯è¿­ä»£": "æ˜¯" if label in [l.strip() for l in iterable_labels] else "å¦"
                    })

                iteration_df = pd.DataFrame(iteration_analysis)

                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"""
                    - è¿­ä»£æ–‡ä»¶æ€»æ¡ç›®æ•°: {iteration_stats['total_in_iteration']}
                    - åŒ¹é…åˆ°åŸæ–‡çš„æ¡ç›®: {iteration_stats['matched_in_original']}
                    - å·²æ›´æ–°çš„ç¿»è¯‘: {iteration_stats['updated_translations']}
                    - ä¸ç¬¦åˆæ ‡å‡†çš„ç¿»è¯‘: {iteration_stats['skipped_not_iterable']}
                    """)

                with col2:
                    st.dataframe(iteration_df)

                if updated_records:
                    df_updated = pd.DataFrame(updated_records)
                    st.subheader("æœ¬æ¬¡è¿­ä»£æ›´æ–°æ˜ç»†")
                    st.dataframe(df_updated)
                else:
                    st.info("æœ¬æ¬¡è¿­ä»£æ²¡æœ‰æ›´æ–°ä»»ä½•æ¡ç›®ï¼ˆæˆ–æ— åŒ¹é…å¯è¿­ä»£æ ‡ç­¾ï¼‰ã€‚")
                # ä¿å­˜æ›´æ–°åçš„è¯‘æ–‡åˆ° session_stateï¼Œä»¥ä¾¿ä¸‹ä¸€è½®å¯¼å‡ºåŸºäºå·²æ¥å—çš„è¯‘æ–‡
                st.session_state.translation_dict = translation_dict
                # ä» pending_keys ä¸­ç§»é™¤å·²è¢«æ¥å—çš„ç¼–å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if updated_records:
                    accepted_keys = [r.get("ç¼–å·") for r in updated_records if r.get("ç¼–å·")]
                    pending = st.session_state.get("pending_keys", [])
                    st.session_state.pending_keys = [k for k in pending if k not in accepted_keys]

        # ---- é‡æ–°è®¡ç®—æœ€ç»ˆ DataFrameï¼ˆä¼˜å…ˆä½¿ç”¨ä¼šè¯ä¸­å·²ä¿å­˜çš„è¯‘æ–‡ï¼‰ ----
        translation_display_dict = st.session_state.get("translation_dict", translation_dict)
        df_result = calculate_length_status(original_dict, translation_display_dict, custom_statuses)

        # ---- ä¸Šä¼ æ–‡ä»¶æ•´ä½“ç»Ÿè®¡ ----
        st.subheader("å­—æ®µç»Ÿè®¡ä¿¡æ¯ï¼ˆåŸæ–‡ vs è¯‘æ–‡ï¼‰")
        stats_df = compute_statistics(df_result, custom_statuses, total_field="åŸæ–‡")
        st.dataframe(
            stats_df.reset_index(drop=True)
                    .style
                    .set_properties(subset=["ç±»å‹"], **{'text-align': 'left'})
                    .set_properties(subset=["æ•°é‡","å æ¯”"], **{'text-align': 'center'})
        )

        # ---- AgGrid æ˜¾ç¤º ----
        st.subheader("ç¿»è¯‘é•¿åº¦æ£€æŸ¥ç»“æœ")
        st.info("ä¸‹è¡¨æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„åŸæ–‡ã€è¯‘æ–‡ã€é•¿åº¦åŠæ ‡ç­¾ï¼Œå¯é€‰æ‹©å¯¼å‡ºè¿‡çŸ­æˆ–è¿‡é•¿å­—æ®µã€‚")

        gb = GridOptionsBuilder.from_dataframe(df_result)
        gb.configure_selection("multiple", use_checkbox=True)
        gb.configure_default_column(filter=True, sortable=True, resizable=True)
        status_colors = {s["name"]: s["color"] for s in custom_statuses}
        cellstyle_jscode = JsCode(f"""
        function(params) {{
            const colors = {json.dumps(status_colors)};
            if (colors[params.value]) {{
                return {{backgroundColor: colors[params.value]}};
            }} else {{
                return {{}};
            }}
        }}
        """)
        gb.configure_column("æ ‡ç­¾", cellStyle=cellstyle_jscode)
        for col in df_result.columns:
            gb.configure_column(col, tooltipField=col)
        grid_options = gb.build()
        AgGrid(df_result, gridOptions=grid_options, height=600, fit_columns_on_grid_load=True,
               enable_enterprise_modules=False, allow_unsafe_jscode=True)

        # ---- å¯¼å‡ºåŠŸèƒ½ ----
        st.subheader("é€‰æ‹©å¯¼å‡ºæ¡ä»¶")
        st.info("é€‰æ‹©è¦å¯¼å‡ºçš„å­—æ®µæ ‡ç­¾ï¼Œå¹¶å¯é€‰æ‹©æ‹†åˆ†å¯¼å‡ºæ–‡ä»¶ã€‚")
        export_checks = {}
        for s in custom_statuses:
            export_checks[s["name"]] = st.checkbox(f"{s['name']}å­—æ®µ", value=(s["name"] in ["è¿‡çŸ­","è¿‡é•¿"]))
        # ===== ä¿®å¤ï¼šä½¿ç”¨æœ€æ–°è¯‘æ–‡åŠ¨æ€è®¡ç®—å¾…ç¿»è¯‘å­—æ®µ =====
        translation_dict_runtime = st.session_state.get("translation_dict", translation_dict)
        df_result_runtime = calculate_length_status(original_dict, translation_dict_runtime, custom_statuses)

        # æ ¹æ®é€‰ä¸­çš„å¯è¿­ä»£æ ‡ç­¾ç­›é€‰ä»éœ€ç¿»è¯‘çš„å­—æ®µ
        export_df_runtime = df_result_runtime[df_result_runtime["æ ‡ç­¾"].isin(
            [name for name, checked in export_checks.items() if checked]
        )]

        st.write(f"å½“å‰ä»éœ€ç¿»è¯‘çš„å­—æ®µæ•°é‡: {len(export_df_runtime)}")

        if not export_df_runtime.empty:
            st.write(f"ç¬¦åˆæ¡ä»¶çš„å­—æ®µæ•°é‡: {len(export_df_runtime)}")
            split_lines = st.number_input("æ¯ä¸ªæ‹†åˆ†æ–‡ä»¶è¡Œæ•°ï¼ˆç•™ç©ºæˆ– 0 è¡¨ç¤ºä¸æ‹†åˆ†ï¼‰", min_value=0, value=0, step=1, key="tab1_split_lines")

            if split_lines > 0:
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, "w") as zip_file:
                    total_lines = len(export_df_runtime)
                    num_parts = (total_lines + split_lines - 1) // split_lines
                    st.write(f"å°†æ‹†åˆ†æˆ {num_parts} ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæœ€å¤š {split_lines} è¡Œã€‚")
                    for i in range(num_parts):
                        part_df = export_df_runtime.iloc[i*split_lines:(i+1)*split_lines]
                        part_txt = "\n".join([f"{row['ç¼–å·']}={row['åŸæ–‡']}" for idx,row in part_df.iterrows()])
                        part_name = f"ç­›é€‰åŸæ–‡_part_{i+1}.txt"
                        zip_file.writestr(part_name, part_txt)
                zip_buffer.seek(0)
                st.download_button(label=f"ä¸‹è½½æ‹†åˆ†åçš„å‹ç¼©åŒ… ({num_parts} ä¸ªæ–‡ä»¶)",
                                   data=zip_buffer.getvalue(),
                                   file_name=f"ç­›é€‰åŸæ–‡æ‹†åˆ†_{int(t.time())}.zip",
                                   mime="application/zip")
            else:
                export_txt = "\n".join([f"{row['ç¼–å·']}={row['åŸæ–‡']}" for idx,row in export_df_runtime.iterrows()])
                st.download_button(label="ä¸‹è½½ç­›é€‰ç»“æœ (.txt)",
                                   data=export_txt,
                                   file_name=f"ç­›é€‰åŸæ–‡_{int(t.time())}.txt",
                                   mime="text/plain")

        # ---- å¯¼å‡ºæœ€æ–°ç‰ˆç¿»è¯‘æ–‡ä»¶ ----
        final_translation_txt = "\n".join([f"{key}={value}" for key,value in st.session_state.get("translation_dict", translation_dict).items()])
        st.download_button(label="å¯¼å‡ºæœ€æ–°ç‰ˆçš„æ€»ç¿»è¯‘æ–‡ä»¶ (.txt)",
                           data=final_translation_txt,
                           file_name="æœ€æ–°ç¿»è¯‘.txt",
                           mime="text/plain")
        
        # workflow é…ç½®
        COZE_TOKEN = "pat_tI3FcbOnw0DsbHF4TYWemJtD1FLLCHYhtO0RBgZMaPHpAxqYnZ4UjAB3QAyItY7w"
        WORKFLOW_ID = "7582900707377446975"
        coze_client = coze(auth=TokenAuth(token=COZE_TOKEN), base_url=COZE_CN_BASE_URL)

        # æ„å»ºç”¨äºç¿»è¯‘çš„æ‰¹æ¬¡ï¼šä¼˜å…ˆä½¿ç”¨ä¼šè¯ä¸­çš„ pending_keysï¼ˆè‹¥ä¸ºç©ºåˆ™ä»¥å½“å‰ç­›é€‰ç»“æœåˆå§‹åŒ–ï¼‰ï¼Œ
        # å¹¶ä¿è¯ pending_keys ä¸å½“å‰ç­›é€‰ç»“æœåŒæ­¥ï¼ˆç§»é™¤å·²è¢«æ ‡è®°ä¸ºåˆæ ¼æˆ–å·²è¢«æ¥å—çš„é”®ï¼‰
        translation_dict_runtime = st.session_state.get("translation_dict", translation_dict)

        # é‡æ–°è®¡ç®—é•¿åº¦æ£€æŸ¥ç»“æœå¹¶ç­›é€‰å¾…ç¿»è¯‘é¡¹ï¼ˆéµå¾ªå½“å‰ export_checksï¼‰
        df_result_runtime = calculate_length_status(original_dict, translation_dict_runtime, custom_statuses)
        export_df_runtime = df_result_runtime[df_result_runtime["æ ‡ç­¾"].isin([name for name, checked in export_checks.items() if checked])]

        export_keys = list(export_df_runtime["ç¼–å·"])
        existing_pending = st.session_state.get("pending_keys", [])
        if not existing_pending:
            # åˆæ¬¡åˆå§‹åŒ– pending_keys
            st.session_state.pending_keys = export_keys
        else:
            # ä¿ç•™åŸæœ‰é¡ºåºï¼Œä½†è¿‡æ»¤æ‰å·²ä¸å†ç¬¦åˆå¯¼å‡ºæ¡ä»¶çš„é”®
            export_key_set = set(export_keys)
            st.session_state.pending_keys = [k for k in existing_pending if k in export_key_set]

        current_pending_keys = st.session_state.pending_keys
        field_objects = [f"{k}={original_dict[k]}" for k in current_pending_keys]
        st.info(f"å¾…ç¿»è¯‘é˜Ÿåˆ—é•¿åº¦: {len(current_pending_keys)}ï¼ˆå°†æŒ‰è¯¥é˜Ÿåˆ—é¡ºåºåˆ†æ‰¹å‘é€ï¼‰")

        # æ„å»º batch
        batch_size = 10
        batches = [field_objects[i:i+batch_size] for i in range(0, len(field_objects), batch_size)]

        DEBUG_MODE = False
        if DEBUG_MODE:
            batches = batches[:2]

        total_batches = len(batches)
        st.info(f"æ‰§è¡Œ {total_batches} æ‰¹æ¬¡")

        language = st.text_input("ç›®æ ‡è¯­è¨€", value="es")
        terminology = st.text_input("æœ¯è¯­è¡¨ï¼ˆå¯é€‰ï¼‰", value="")

        def run_batch(batch, batch_index):
            results = []
            raw_events = []
            try:
                stream = coze_client.workflows.runs.stream(
                    workflow_id=WORKFLOW_ID,
                    parameters={
                        "url": batch,  # ç›´æ¥ä¼ å­—æ®µæ•°ç»„
                        "language": language,
                        "terminology": terminology
                    }
                )
                for event in stream:
                    raw_events.append(repr(event))
                    if event.event == WorkflowEventType.MESSAGE:
                        content = getattr(event.message, "content", None)
                        if content:
                            try:
                                results.append(json.loads(content))
                            except Exception:
                                results.append(content)
            except Exception as e:
                raw_events.append(f"Batch {batch_index+1} è°ƒç”¨å¤±è´¥ï¼š{e}")
            return batch_index, results, raw_events
        
        if st.button("å¼€å§‹è°ƒç”¨ Workflowï¼ˆå¹¶è¡Œ + å®æ—¶è¿›åº¦ï¼‰"):
            progress_bar = st.progress(0)
            status_text = st.empty()

            all_results = [None]*total_batches
            all_raw_events = [None]*total_batches

            with ThreadPoolExecutor(max_workers=total_batches) as executor:
                futures = [executor.submit(run_batch, batch, idx) for idx, batch in enumerate(batches)]
                for i, future in enumerate(futures):
                    idx, results, raw_events = future.result()
                    all_results[idx] = results
                    all_raw_events[idx] = raw_events

                    progress = int(((i+1)/total_batches) * 100)
                    progress_bar.progress(progress)
                    status_text.text(f"å·²å®Œæˆ {i+1}/{total_batches} æ‰¹æ¬¡")

                    # st.write(f"### æ‰¹æ¬¡ {idx+1} è¿”å›ç»“æœï¼š")
                    # st.text(json.dumps(results, ensure_ascii=False, indent=2))

            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
            workflow_results = [item for batch in all_results if batch for item in batch]
            workflow_raw_events = [item for batch in all_raw_events if batch for item in batch]

            # â­â­ å…³é”®ï¼šè°ƒç”¨è§£æå‡½æ•° â­â­

            st.session_state.has_workflow_result = True
            parsed_results = parse_workflow_results(workflow_results)
            st.session_state.iteration_dict = parsed_results
            can_iterate = bool(iteration_file) or st.session_state.has_workflow_result

            # å½“æŒ‰é’®è¢«ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ process_iteration å¹¶å±•ç¤ºç»“æœ
            if st.button("ä½¿ç”¨å½“å‰ç»“æœæ‰§è¡Œè¿­ä»£", disabled=not can_iterate):
                iteration_dict_runtime = st.session_state.iteration_dict
                # ä½¿ç”¨ä¼šè¯ä¸­ä¿å­˜çš„æœ€æ–°è¯‘æ–‡ï¼Œé¿å…ä½¿ç”¨æ—§çš„æœ¬åœ°å˜é‡è¦†ç›–å·²ç»æ¥å—çš„è¯‘æ–‡
                translation_runtime = st.session_state.get("translation_dict", translation_dict)
                translation_dict, updated_records, iteration_stats, iteration_labels_count = process_iteration(
                    original_dict, translation_runtime, iteration_dict_runtime, iterable_labels, custom_statuses
                )

                if iteration_stats["total_in_iteration"] > 0:
                    st.success("å·²åº”ç”¨ Workflow è§£æç»“æœå¹¶æ›´æ–°è¯‘æ–‡å­—å…¸ã€‚")
                    # æ˜¾ç¤ºç»Ÿè®¡
                    iteration_analysis = []
                    for label, count in iteration_labels_count.items():
                        percentage = (count / iteration_stats["matched_in_original"] * 100) if iteration_stats["matched_in_original"] > 0 else 0
                        iteration_analysis.append({
                            "æ ‡ç­¾": label,
                            "æ•°é‡": count,
                            "å æ¯”": f"{percentage:.1f}%",
                            "æ˜¯å¦å¯è¿­ä»£": "æ˜¯" if label in [l.strip() for l in iterable_labels] else "å¦"
                        })
                    st.dataframe(pd.DataFrame(iteration_analysis))

                    if updated_records:
                        st.subheader("å·²æ›´æ–°æ¡ç›®ï¼ˆæ¥è‡ª Workflowï¼‰")
                        st.dataframe(pd.DataFrame(updated_records))
                    
                    # â­â­ å…³é”®ï¼šè¿­ä»£å®Œæˆåé‡æ–°è®¡ç®—æœ€ç»ˆè¡¨æ ¼ä¸ç»Ÿè®¡ï¼Œä¸ºä¸‹ä¸€è½® workflow åšå‡†å¤‡ â­â­
                    st.divider()
                    st.subheader("ç¬¬äºŒè½®å‡†å¤‡ï¼šé‡æ–°è®¡ç®—ç»Ÿè®¡ä¸å¯¼å‡ºå†…å®¹")
                    
                    # é‡æ–°è®¡ç®—é•¿åº¦æ£€æŸ¥ç»“æœ
                    df_result_new = calculate_length_status(original_dict, translation_dict, custom_statuses)
                    
                    # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    stats_df_new = compute_statistics(df_result_new, custom_statuses, total_field="åŸæ–‡")
                    st.write("æ›´æ–°åçš„å­—æ®µç»Ÿè®¡ï¼š")
                    st.dataframe(
                        stats_df_new.reset_index(drop=True)
                                .style
                                .set_properties(subset=["ç±»å‹"], **{'text-align': 'left'})
                                .set_properties(subset=["æ•°é‡","å æ¯”"], **{'text-align': 'center'})
                    )
                    
                    # é‡æ–°åº”ç”¨å¯¼å‡ºæ¡ä»¶ï¼Œè¿‡æ»¤å‡ºéœ€è¦ç»§ç»­ç¿»è¯‘çš„å†…å®¹
                    export_df_new = df_result_new[df_result_new["æ ‡ç­¾"].isin([name for name, checked in export_checks.items() if checked])]
                    new_field_count = len(export_df_new)
                    
                    if new_field_count > 0:
                        st.info(f"âœ… å‘ç° **{new_field_count}** ä¸ªä»éœ€ç¿»è¯‘çš„å­—æ®µï¼ˆè¿‡çŸ­/è¿‡é•¿ï¼‰ï¼Œå¯ç»§ç»­è¿è¡Œä¸‹ä¸€æ‰¹ Workflowã€‚")
                        st.write("æ–°ä¸€æ‰¹éœ€è¦ç¿»è¯‘çš„å­—æ®µç¤ºä¾‹ï¼ˆå‰5æ¡ï¼‰ï¼š")
                        st.dataframe(export_df_new.head(5)[["ç¼–å·", "åŸæ–‡", "è¯‘æ–‡", "æ ‡ç­¾"]])
                    else:
                        st.success("ğŸ‰ æ‰€æœ‰å­—æ®µå·²è¾¾åˆ°åˆæ ¼æ ‡å‡†ï¼Œæ— éœ€å†æ¬¡è¿­ä»£ï¼")
                else:
                    st.info("Workflow ç»“æœä¸­æœªå‘ç°å¯åº”ç”¨çš„è¿­ä»£å†…å®¹ã€‚")

                # ä¿å­˜æ›´æ–°åçš„è¯‘æ–‡åˆ° session_stateï¼Œä¾›ä¸‹ä¸€è½®ä½¿ç”¨
                st.session_state.translation_dict = translation_dict
                # ä» pending_keys ä¸­ç§»é™¤å·²è¢«æ¥å—çš„ç¼–å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if updated_records:
                    accepted_keys = [r.get("ç¼–å·") for r in updated_records if r.get("ç¼–å·")]
                    pending = st.session_state.get("pending_keys", [])
                    st.session_state.pending_keys = [k for k in pending if k not in accepted_keys]

            st.success("Workflow æ‰§è¡Œå®Œæˆ")

            st.subheader("Workflow è¾“å‡ºç»“æœï¼ˆåŸå§‹ï¼‰")
            st.text_area(
                "Raw",
                json.dumps(workflow_results, ensure_ascii=False, indent=2),
                height=300
            )

            st.subheader("è§£æåçš„å¯è¿­ä»£å†…å®¹")
            st.text_area(
                "Parsed",
                json.dumps(parsed_results, ensure_ascii=False, indent=2),
                height=300
            )
